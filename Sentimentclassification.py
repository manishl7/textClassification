# -*- coding: utf-8 -*-
"""project1_sentimentcalssification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eL3Z6itQgD99Ixz5AbQZTRBAZt5RPKsl
"""

from google.colab import drive
drive.mount("/content/drive",force_remount=True)

cd /content/drive/My Drive/RNN/

!unzip 2477_4140_bundle_archive.zip

import pandas as pd
import numpy as np
import pickle
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout
from sklearn.preprocessing import LabelBinarizer
import sklearn.datasets as skds
from pathlib import Path

#importing CSV file,dataframing,sorting df with desirable columns 
import csv
import sys
import pandas as pd

texts=pd.read_csv("/content/drive/My Drive/RNN/sentiment.csv",skiprows=0,encoding='latin-1')
#dataset link :https://www.kaggle.com/kazanova/sentiment140
data=texts
data.columns=['category','ids','date','flag','user','text'] #UTF-8 couldn't decode data hence when latin-1 was used , Columns weren't displayed so new column_names were assigned
#print(data)
#data1=data
#data1.to_csv("/content/drive/My Drive/RNN/data1.csv")
data1=pd.read_csv("/content/drive/My Drive/RNN/data1.csv",skiprows=0,encoding='latin-1') #creating a new csv file with proper cols
data1

#category0-neg , category 2- neutral , category -4 positive
#print(data.head(5))



# lets take 80% data as training and remaining 20% for test.
train_size = int(len(data1) * .8)
 
train_posts = data1['text'][:train_size]
train_tags = data1['category'][:train_size]
train_files_names = data1['user'][:train_size]
 
test_posts = data1['text'][train_size:]
test_tags = data1['category'][train_size:]
test_files_names = data1['user'][train_size:]

num_labels = 3
vocab_size = 10
batch_size = 100
 
# define Tokenizer with Vocab Size
tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train_posts)
 
x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')
x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')
 
encoder = LabelBinarizer()
encoder.fit(train_tags)
y_train = encoder.transform(train_tags)
y_test = encoder.transform(test_tags)

encoder = LabelBinarizer()
encoder.fit(train_tags)
y_train = encoder.transform(train_tags)
y_test = encoder.transform(test_tags)

model = Sequential()
model.add(Dense(512, input_shape=(vocab_size,)))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.3))
model.add(Dense(num_labels))
model.add(Activation('softmax'))
model.summary()
 
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=30,
                    verbose=1,
                    validation_split=0.1)

#error :     ValueError: Shapes (None, 1) and (None, 3) are incompatible

